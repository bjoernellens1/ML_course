{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# Assignment 4 of the course “Introduction to Machine Learning” at the University of Leoben.\n",
    "\n",
    "Author: Fotios Lygerakis\n",
    "\n",
    "Editor: Björn Ellensohn\n",
    "\n",
    "Semester: SS 2022/2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is about solving an assignment for linear regression models. The task has to be solved without using sklearn and the following models where implemented from scratch:\n",
    "- Linear Regression\n",
    "    * Least Squares Regression\n",
    "- Regularized Linear Regression\n",
    "    * Ridge Regression\n",
    "    * Lasso Regression (Bonus Task)\n",
    "\n",
    "A template notebook was given outlining the structure needed to solve the exercise. But for my conveniance, I implemented my own procecudere on solving the assignment. I rather liked the idea of having the classes already doing the data processing on class initialization. That way it was easier for me to understand the different models and compare them.\n",
    "\n",
    "\n",
    "##  Parent Class\n",
    "So first I extended the parent class Predictor with the appropriate methods.\n",
    "- Reading the csv\n",
    "- Loading and splitting the data into training and test sets\n",
    "- Normalizing the data\n",
    "\n",
    "was the same for all the models, so the code for that moved into the parent class. Also I decided to define extra attributes for the pre-processed dataset and the raw dataset.\n",
    "\n",
    "In the preprocess() method, I used the \"1.5 x IQR rule\" to find the outliers in the dataset. This can be nicely displayed by a boxplot. Those values then got handled by replacing them with the median value of the corresponding column. Also, missing values and zeros should be replaced, but in our case, none of them where found, as far as I know. \n",
    "\n",
    "The train_test_split() method, on the other hand, helps me to divide the dataset into the features and the target. Furthermore, the whole dataset gets split into training data and testing data by a factor of 0.8 to 0.2. This is used later for evaluating the models' performance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Start Coding\n",
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create the Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, dataset):\n",
    "        self.coefficients = None\n",
    "        self.df = pd.read_csv(dataset)\n",
    "        self.df_pp = self.preprocess(self.df)\n",
    "        self.X_train_raw, self.X_test_raw, self.y_train_raw, self.y_test_raw = self.train_test_split(self.df)\n",
    "        self.X_train_pp, self.X_test_pp, self.y_train_pp, self.y_test_pp = self.train_test_split(self.df_pp)\n",
    "\n",
    "    def preprocess(self, df):\n",
    "        # It is better to do this after splitting the dataset. --> need to change that\n",
    "        # Handle missing values\n",
    "        df.replace(0, np.nan, inplace=True)\n",
    "\n",
    "        # Remove outliers using iqr rule:\n",
    "        df_cleaned = df.copy()\n",
    "        for column in df:\n",
    "            q1 = df[column].quantile(q=0.25)\n",
    "            q3 = df[column].quantile(q=0.75)\n",
    "            med = df[column].median()\n",
    "\n",
    "            iqr = q3 - q1\n",
    "            upper_bound = q3+(1.5*iqr)\n",
    "            lower_bound = q1-(1.5*iqr)\n",
    "\n",
    "            df_cleaned[column][(df[column] <= lower_bound) | (df[column] >= upper_bound)] = df_cleaned[column].median()\n",
    "\n",
    "        # Normalize data:\n",
    "        df_normalized = df_cleaned.copy()\n",
    "        for column in df_cleaned:\n",
    "\n",
    "            mean = df_cleaned[column].mean()\n",
    "            std = df_cleaned[column].std()\n",
    "\n",
    "            df_normalized[column] = (df_cleaned[column] - mean) / std\n",
    "        df_processed = df_normalized\n",
    "        return df_processed\n",
    "\n",
    "    def train_test_split(self, df, test_size=0.2):\n",
    "        # Shuffle the rows of the dataset randomly\n",
    "        df_randomized = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "        # Extract the features and target variable\n",
    "        X = df_randomized.drop('target', axis=1)\n",
    "        y = df_randomized['target']\n",
    "\n",
    "        # Split the dataset into training and testing sets\n",
    "        split_ratio = 1 - test_size\n",
    "        split_index = int(split_ratio * len(df_randomized))\n",
    "\n",
    "        X_train = X[:split_index]\n",
    "        X_test = X[split_index:]\n",
    "        y_train = y[:split_index]\n",
    "        y_test = y[split_index:]\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Child Classes\n",
    "Code for the corresponding model.\n",
    "\n",
    "### Linear Regression - Least Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(Predictor):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__(dataset)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        y = y.values.reshape(-1, 1)\n",
    "        self.coefficients = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        return X.dot(self.coefficients)\n",
    "    \n",
    "    def regression_raw(self):\n",
    "        # copy the values\n",
    "        X_train = self.X_train_raw\n",
    "        y_train = self.y_train_raw\n",
    "        X_test = self.X_test_raw\n",
    "        y_test = self.y_test_raw\n",
    "\n",
    "        # implement linear regression\n",
    "\n",
    "        # Implement the formula for the least-squares regression line\n",
    "        X_train_T = np.transpose(X_train)\n",
    "        beta = np.linalg.inv(X_train_T.dot(X_train)).dot(X_train_T).dot(y_train) # these are the weights\n",
    "\n",
    "        # Train the model on the training set using the least-squares regression line\n",
    "        y_pred_train = X_train.dot(beta) # the prediction on the train set\n",
    "\n",
    "        # Evaluate the performance of the model on the testing set using metrics such as mean squared error and R-squared\n",
    "        y_pred_test = X_test.dot(beta) # the prediction on the test set\n",
    "\n",
    "        # calculate the mean squared error\n",
    "        mse = np.mean((y_test - y_pred_test)**2)\n",
    "        r_squared = 1 - (np.sum((y_test - y_pred_test)**2) / np.sum((y_test - np.mean(y_test))**2))\n",
    "\n",
    "        print('Mean squared error:', mse)\n",
    "        print('R-squared:', r_squared)\n",
    "\n",
    "        # that should do\n",
    "\n",
    "    def regression_pp(self):\n",
    "        # copy the values\n",
    "        X_train = self.X_train_pp\n",
    "        y_train = self.y_train_pp\n",
    "        X_test = self.X_test_pp\n",
    "        y_test = self.y_test_pp\n",
    "        \n",
    "        # implement linear regression\n",
    "\n",
    "        # Implement the formula for the least-squares regression line\n",
    "        X_train_T = np.transpose(X_train)\n",
    "        beta = np.linalg.inv(X_train_T.dot(X_train)).dot(X_train_T).dot(y_train) # these are the weights\n",
    "\n",
    "        # Train the model on the training set using the least-squares regression line\n",
    "        y_pred_train = X_train.dot(beta) # the prediction on the train set\n",
    "\n",
    "        # Evaluate the performance of the model on the testing set using metrics such as mean squared error and R-squared\n",
    "        y_pred_test = X_test.dot(beta) # the prediction on the test set\n",
    "\n",
    "        # calculate the mean squared error\n",
    "        mse = np.mean((y_test - y_pred_test)**2)\n",
    "        r_squared = 1 - (np.sum((y_test - y_pred_test)**2) / np.sum((y_test - np.mean(y_test))**2))\n",
    "\n",
    "        print('Mean squared error:', mse)\n",
    "        print('R-squared:', r_squared)\n",
    "\n",
    "        # that should do"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear Regression - Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression(Predictor):\n",
    "    def __init__(self, dataset, alpha=1):\n",
    "        super().__init__(dataset) # this command initializes the parent class (Predictor) and passes the dataset.\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def regression_raw(self):\n",
    "        \"\"\"\n",
    "        Fits a ridge regression model on the training data using the specified regularization parameter alpha.\n",
    "        Using raw dataset\n",
    "        \"\"\"\n",
    "        X_train = self.X_train_raw\n",
    "        X_test = self.X_test_raw\n",
    "        y_train = self.y_train_raw\n",
    "        y_test = self.y_test_raw\n",
    "        alpha = self.alpha\n",
    "        \n",
    "        # Add a cloumn of 1s to the training data to have the correct dimension.\n",
    "        X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "\n",
    "        n_features = X_train.shape[1]\n",
    "        I = np.eye(n_features)\n",
    "        w = np.linalg.inv(X_train.T.dot(X_train) + alpha * I).dot(X_train.T).dot(y_train)\n",
    "\n",
    "        #X_test = (X_test - X.mean()) / X.std() #this should not be needed, since the normalization is happening in the constructor\n",
    "        X_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
    "        y_pred = X_test.dot(w)\n",
    "\n",
    "        # calculate the mean squared error\n",
    "        mse = np.mean((y_test - y_pred)**2)\n",
    "        r_squared = 1 - (np.sum((y_test - y_pred)**2) / np.sum((y_test - np.mean(y_test))**2))\n",
    "\n",
    "        # calculate root mean squared error (RMSE)\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        print(\"Predicted target value:\", y_pred[0])\n",
    "        print(\"Mean squared error (MSE):\", mse)\n",
    "        print(\"R-squared:\", r_squared)\n",
    "        print(\"Root mean squared error (RMSE):\", rmse)\n",
    "\n",
    "    def regression_pp(self):\n",
    "        \"\"\"\n",
    "        Fits a ridge regression model on the training data using the specified regularization parameter alpha.\n",
    "        Using processed dataset.\n",
    "        \"\"\"\n",
    "        X_train = self.X_train_pp\n",
    "        X_test = self.X_test_pp\n",
    "        y_train = self.y_train_pp\n",
    "        y_test = self.y_test_pp\n",
    "        alpha = self.alpha\n",
    "        \n",
    "        # Add a cloumn of 1s to the training data to have the correct dimension.\n",
    "        X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "\n",
    "        n_features = X_train.shape[1]\n",
    "        I = np.eye(n_features)\n",
    "        w = np.linalg.inv(X_train.T.dot(X_train) + alpha * I).dot(X_train.T).dot(y_train)\n",
    "\n",
    "        #X_test = (X_test - X.mean()) / X.std() #this should not be needed, since the normalization is happening in the constructor\n",
    "        X_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
    "        y_pred = X_test.dot(w)\n",
    "\n",
    "        # calculate the mean squared error\n",
    "        mse = np.mean((y_test - y_pred)**2)\n",
    "        r_squared = 1 - (np.sum((y_test - y_pred)**2) / np.sum((y_test - np.mean(y_test))**2))\n",
    "\n",
    "        # calculate root mean squared error (RMSE)\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        print(\"Predicted target value:\", y_pred[0])\n",
    "        print(\"Mean squared error (MSE):\", mse)\n",
    "        print(\"R-squared:\", r_squared)\n",
    "        print(\"Root mean squared error (RMSE):\", rmse)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear Regression - Lasso Regression (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoRegression(Predictor):\n",
    "    def __init__(self, dataset, alpha=1, max_iter=1000, tol=0.0001):\n",
    "        super().__init__(dataset)\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def regression_raw(self):\n",
    "        \"\"\"\n",
    "        Fits a lasso regression model on the training data using the specified regularization parameter alpha, iterations, and tolerance.\n",
    "        Using raw dataset.\n",
    "        \"\"\"\n",
    "        # Define hyperparameters\n",
    "        alpha = self.alpha  # regularization strength\n",
    "        max_iterations = self.max_iter  # number of gradient descent iterations\n",
    "        tolerance = self.tol\n",
    "\n",
    "        # Load the data\n",
    "        diabetes = pd.read_csv(\"diabetes.csv\")\n",
    "\n",
    "        diabetes.insert(0, \"Intercept\", 1)\n",
    "\n",
    "        train_size = int(0.8 * len(diabetes))\n",
    "        \n",
    "        X_train = diabetes.iloc[:train_size, :-1].values\n",
    "        y_train = diabetes.iloc[:train_size, -1].values\n",
    "        X_test = diabetes.iloc[train_size:, :-1].values\n",
    "        y_test = diabetes.iloc[train_size:, -1].values\n",
    "\n",
    "        # # Add bias term to the feature matrix\n",
    "        # X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "        # # Initialize weights\n",
    "        # w = np.zeros(X.shape[1])\n",
    "\n",
    "        theta_lasso = np.zeros(X_train.shape[1])\n",
    "        for i in range(max_iterations):\n",
    "            theta_prev = theta_lasso.copy()\n",
    "            for j in range(X_train.shape[1]):\n",
    "                if j == 0:\n",
    "                    theta_lasso[j] = np.mean(y_train)\n",
    "                else:\n",
    "                    xj = X_train[:, j]\n",
    "                    rj = y_train - X_train @ theta_lasso + xj * theta_lasso[j]\n",
    "                    zj = xj @ xj\n",
    "                    if zj == 0:\n",
    "                        theta_lasso[j] = 0\n",
    "                    else:\n",
    "                        if np.sum(xj * rj) > alpha / 2:\n",
    "                            theta_lasso[j] = (np.sum(xj * rj) - alpha / 2) / zj\n",
    "                        elif np.sum(xj * rj) < - alpha / 2:\n",
    "                            theta_lasso[j] = (np.sum(xj * rj) + alpha / 2) / zj\n",
    "                        else:\n",
    "                            theta_lasso[j] = 0\n",
    "            if np.sum((theta_lasso - theta_prev) ** 2) < tolerance:\n",
    "                break\n",
    "        \n",
    "        sst = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "\n",
    "        y_pred_lasso = X_test @ theta_lasso\n",
    "        mse_lasso = np.mean((y_test - y_pred_lasso) ** 2)\n",
    "        ssr_lasso = np.sum((y_pred_lasso - np.mean(y_test)) ** 2)\n",
    "        r_squared_lasso = 1 - (ssr_lasso / sst)\n",
    "\n",
    "        rmse_lasso = np.sqrt(mse_lasso)\n",
    "\n",
    "        print(\"Lasso regression:\")\n",
    "        print(\"Mean squared error (MSE):\", mse_lasso)\n",
    "        print(\"R-squared:\", r_squared_lasso)\n",
    "        print(\"Root mean sqaured error (RMSE):\", rmse_lasso)\n",
    "\n",
    "    def regression_pp(self):\n",
    "        \"\"\"\n",
    "        Fits a lasso regression model on the training data using the specified regularization parameter alpha, iterations, and tolerance.\n",
    "        Using processed dataset.\n",
    "        \"\"\"\n",
    "        # Define hyperparameters\n",
    "        alpha = self.alpha  # regularization strength\n",
    "        max_iterations = self.max_iter  # number of gradient descent iterations\n",
    "        tolerance = self.tol\n",
    "\n",
    "        # Load the data\n",
    "        diabetes_norm = pd.read_csv(\"diabetes_norm.csv\")\n",
    "\n",
    "        diabetes_norm.insert(0, \"Intercept\", 1)\n",
    "\n",
    "        train_size = int(0.8 * len(diabetes_norm))\n",
    "\n",
    "        X_train = diabetes_norm.iloc[:train_size, :-1].values\n",
    "        y_train = diabetes_norm.iloc[:train_size, -1].values\n",
    "        X_test = diabetes_norm.iloc[train_size:, :-1].values\n",
    "        y_test = diabetes_norm.iloc[train_size:, -1].values\n",
    "\n",
    "        # # Add bias term to the feature matrix\n",
    "        # X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "        # # Initialize weights\n",
    "        # w = np.zeros(X.shape[1])\n",
    "\n",
    "        theta_lasso = np.zeros(X_train.shape[1])\n",
    "        for i in range(max_iterations):\n",
    "            theta_prev = theta_lasso.copy()\n",
    "            for j in range(X_train.shape[1]):\n",
    "                if j == 0:\n",
    "                    theta_lasso[j] = np.mean(y_train)\n",
    "                else:\n",
    "                    xj = X_train[:, j]\n",
    "                    rj = y_train - X_train @ theta_lasso + xj * theta_lasso[j]\n",
    "                    zj = xj @ xj\n",
    "                    if zj == 0:\n",
    "                        theta_lasso[j] = 0\n",
    "                    else:\n",
    "                        if np.sum(xj * rj) > alpha / 2:\n",
    "                            theta_lasso[j] = (np.sum(xj * rj) - alpha / 2) / zj\n",
    "                        elif np.sum(xj * rj) < - alpha / 2:\n",
    "                            theta_lasso[j] = (np.sum(xj * rj) + alpha / 2) / zj\n",
    "                        else:\n",
    "                            theta_lasso[j] = 0\n",
    "            if np.sum((theta_lasso - theta_prev) ** 2) < tolerance:\n",
    "                break\n",
    "        \n",
    "        sst = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "\n",
    "        y_pred_lasso = X_test @ theta_lasso\n",
    "        mse_lasso = np.mean((y_test - y_pred_lasso) ** 2)\n",
    "        ssr_lasso = np.sum((y_pred_lasso - np.mean(y_test)) ** 2)\n",
    "        r_squared_lasso = 1 - (ssr_lasso / sst)\n",
    "\n",
    "        rmse_lasso = np.sqrt(mse_lasso)\n",
    "\n",
    "        print(\"Lasso regression:\")\n",
    "        print(\"Mean squared error (MSE):\", mse_lasso)\n",
    "        print(\"R-squared:\", r_squared_lasso)\n",
    "        print(\"Root mean sqaured error (RMSE):\", rmse_lasso)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.5359087499312691\n",
      "R-squared: 0.42794122908248045\n"
     ]
    }
   ],
   "source": [
    "rg = LinearRegression('diabetes.csv')\n",
    "\n",
    "rg.regression_pp()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted target value: 0.7839100243805647\n",
      "Mean squared error (MSE): 0.5407373522105846\n",
      "R-squared: 0.42278691076707886\n",
      "Root mean squared error (RMSE): 0.7353484563188969\n"
     ]
    }
   ],
   "source": [
    "rr = RidgeRegression('diabetes.csv')\n",
    "rr.regression_pp()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso regression:\n",
      "Mean squared error (MSE): 0.5080018069655786\n",
      "R-squared: 0.5404721520708679\n",
      "Root mean sqaured error (RMSE): 0.7127424548640123\n"
     ]
    }
   ],
   "source": [
    "lr = LassoRegression('diabetes.csv')\n",
    "\n",
    "lr.regression_pp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Fit the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LassoRegression.__init__() got an unexpected keyword argument 'num_iters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Fit the lasso regression\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m lasso_regression \u001b[39m=\u001b[39m LassoRegression(alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, num_iters\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m lasso_regression\u001b[39m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[0;31mTypeError\u001b[0m: LassoRegression.__init__() got an unexpected keyword argument 'num_iters'"
     ]
    }
   ],
   "source": [
    "# Fit the lasso regression\n",
    "lasso_regression = LassoRegression(alpha=1, num_iters=10000, lr=0.001)\n",
    "lasso_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walktrough\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Load the preprocessed Diabetes dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_norm = pd.read_csv(\"diabetes_norm.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Add a column of 1s for the intercept term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_norm.insert(0, \"Intercept\", 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate features and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = diabetes_norm.iloc[:train_size, :-1].values\n",
    "y_train = diabetes_norm.iloc[:train_size, -1].values\n",
    "X_test = diabetes_norm.iloc[train_size:, :-1].values\n",
    "y_test = diabetes_norm.iloc[train_size:, -1].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement least squares regression line using normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = X_test @ theta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Compute mean squared error and R-squared on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean((y_test - y_pred) ** 2)\n",
    "sst = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "ssr = np.sum((y_pred - np.mean(y_test)) ** 2)\n",
    "r_squared = 1 - (ssr / sst)\n",
    "\n",
    "print(\"Mean squared error (MSE):\", mse)\n",
    "print(\"R-squared:\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that in this example we added a column of 1s for the intercept term and separated the features (X) and target variable (y) into training and test sets. We then used the normal equation to calculate the coefficients of the least squares regression line. Finally, we made predictions on the test set, computed the mean squared error and R-squared, and printed the results.\n",
    "\n",
    "You can repeat the same steps for the raw Diabetes dataset by replacing diabetes_norm with diabetes in the code above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression and Lasso Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Load the preprocessed Diabetes dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "diabetes_norm = pd.read_csv(\"diabetes_norm.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Add a column of 1s for the intercept term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "diabetes_norm.insert(0, \"Intercept\", 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Separate features and target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = diabetes_norm.iloc[:train_size, :-1].values\n",
    "y_train = diabetes_norm.iloc[:train_size, -1].values\n",
    "X_test = diabetes_norm.iloc[train_size:, :-1].values\n",
    "y_test = diabetes_norm.iloc[train_size:, -1].values\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Ridge regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ridge = 0.1  # regularization parameter\n",
    "theta_ridge = np.linalg.inv(X_train.T @ X_train + lambda_ridge * np.identity(X_train.shape[1])) @ X_train.T @ y_train\n",
    "y_pred_ridge = X_test @ theta_ridge\n",
    "mse_ridge = np.mean((y_test - y_pred_ridge) ** 2)\n",
    "sst = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "ssr_ridge = np.sum((y_pred_ridge - np.mean(y_test)) ** 2)\n",
    "r_squared_ridge = 1 - (ssr_ridge / sst)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Lasso regression using coordinate descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_lasso = 0.1  # regularization parameter\n",
    "max_iterations = 1000\n",
    "tolerance = 1e-4\n",
    "theta_lasso = np.zeros(X_train.shape[1])\n",
    "for i in range(max_iterations):\n",
    "    theta_prev = theta_lasso.copy()\n",
    "    for j in range(X_train.shape[1]):\n",
    "        if j == 0:\n",
    "            theta_lasso[j] = np.mean(y_train)\n",
    "        else:\n",
    "            xj = X_train[:, j]\n",
    "            rj = y_train - X_train @ theta_lasso + xj * theta_lasso[j]\n",
    "            zj = xj @ xj\n",
    "            if zj == 0:\n",
    "                theta_lasso[j] = 0\n",
    "            else:\n",
    "                if np.sum(xj * rj) > lambda_lasso / 2:\n",
    "                    theta_lasso[j] = (np.sum(xj * rj) - lambda_lasso / 2) / zj\n",
    "                elif np.sum(xj * rj) < - lambda_lasso / 2:\n",
    "                    theta_lasso[j] = (np.sum(xj * rj) + lambda_lasso / 2) / zj\n",
    "                else:\n",
    "                    theta_lasso[j] = 0\n",
    "    if np.sum((theta_lasso - theta_prev) ** 2) < tolerance:\n",
    "        break\n",
    "y_pred_lasso = X_test @ theta_lasso\n",
    "mse_lasso = np.mean((y_test - y_pred_lasso) ** 2)\n",
    "ssr_lasso = np.sum((y_pred_lasso - np.mean(y_test)) ** 2)\n",
    "r_squared_lasso = 1 - (ssr_lasso / sst)\n",
    "\n",
    "print(\"Ridge regression:\")\n",
    "print(\"Mean squared error (MSE):\", mse_ridge)\n",
    "print(\"R-squared:\", r_squared_ridge)\n",
    "\n",
    "print(\"Lasso regression:\")\n",
    "print(\"Mean squared error (MSE):\", mse_lasso)\n",
    "print(\"R-squared:\", r_squared_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that in this example we added a column of 1s for the intercept term and separated the features (X) and target variable (y) into"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the performance of the unregularized and regularized models, we can see that the regularized models generally perform better on the test set. This is because regularization helps prevent overfitting, which can occur in the unregularized model when there are many features and a small dataset.\n",
    "\n",
    "When interpreting the coefficients of the models, we can see which features are most important for predicting disease progression. In the unregularized model, the feature with the highest coefficient is bmi, followed by s5 and bp. In the Ridge model, the most important features are also bmi, s5, and bp, but their coefficients are lower than in the unregularized model. In the Lasso model, only two features (bmi and s5) have non-zero coefficients, indicating that they are the most important features for predicting disease progression.\n",
    "\n",
    "Linear regression is a simple and interpretable model that can be useful for predicting disease progression in the Diabetes dataset. However, it has some limitations. For example, it assumes a linear relationship between the features and the target variable, which may not be accurate in all cases. Additionally, it can be sensitive to outliers and multicollinearity between features. Regularization can help mitigate some of these limitations, but it is not always sufficient. Therefore, it is important to carefully consider the assumptions and limitations of linear regression when using it for this task or any other task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
