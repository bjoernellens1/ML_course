{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Assignment 4 of the course “Introduction to Machine Learning” at the University of Leoben.\n",
    "Author: Fotios Lygerakis\n",
    "Semester: SS 2022/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create the Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self):\n",
    "        self.coefficients = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "class LinearRegression(Predictor):\n",
    "    def __init__(self):\n",
    "        self.coefficients = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        y = y.values.reshape(-1, 1)\n",
    "        self.coefficients = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        return X.dot(self.coefficients)\n",
    "\n",
    "class RidgeRegression(Predictor):\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha\n",
    "        self.coefficients = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        y = y.values.reshape(-1, 1)\n",
    "        n_features = X.shape[1]\n",
    "        I = np.identity(n_features)\n",
    "        I[0, 0] = 0\n",
    "        self.coefficients = np.linalg.inv(X.T.dot(X) + self.alpha * I).dot(X.T).dot(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        return X.dot(self.coefficients)\n",
    "\n",
    "class LassoRegression(Predictor):\n",
    "    def __init__(self, alpha=1, max_iter=1000, tol=0.0001):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.coefficients = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        y = y.values.reshape(-1, 1)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.coefficients = np.zeros((n_features, 1))\n",
    "        for _ in range(self.max_iter):\n",
    "            coefficients_old = np.copy(self.coefficients)\n",
    "            for j in range(n_features):\n",
    "                X_j = X[:, j].reshape(-1, 1)\n",
    "                y_pred = X.dot(self.coefficients) - X_j.dot(self.coefficients[j])\n",
    "                rho = X_j.T.dot(y - y_pred)\n",
    "                if j == 0:\n",
    "                    self.coefficients[j] = rho\n",
    "                else:\n",
    "                    if rho < -self.alpha/2:\n",
    "                        self.coefficients[j] = rho + self.alpha/2\n",
    "                    elif rho > self.alpha/2:\n",
    "                        self.coefficients[j] = rho - self.alpha/2\n",
    "                    else:\n",
    "                        self.coefficients[j] = 0\n",
    "            if np.max(np.abs(self.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Data Preprocessing and Data loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    # Handle missing values\n",
    "    df.replace(0, np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Remove outliers using Z-score\n",
    "    z_scores = (df - df.mean()) / df.std()\n",
    "    df = df[(z_scores.abs() < 3).all(axis=1)]\n",
    "\n",
    "    # Normalize the data\n",
    "    df_norm = (df - df.mean()) / df.std()\n",
    "\n",
    "def train_test_split(X, y, test_size=0.2):\n",
    "    # Split the data into training and test sets\n",
    "    train_size = int(0.8 * len(diabetes_norm))\n",
    "    train_set = diabetes_norm[:train_size]\n",
    "    test_set = diabetes_norm[train_size:]\n",
    "\n",
    "    # Separate features and target variable\n",
    "    X_train = train_set.iloc[:, :-1]\n",
    "    y_train = train_set.iloc[:, -1]\n",
    "    X_test = test_set.iloc[:, :-1]\n",
    "    y_test = test_set.iloc[:, -1]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "df = preprocess(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m load_data()\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "X_train, X_test, y_train, y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Fit the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the linear regression\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X_train, y_train)\n",
    "\n",
    "# Fit the linear regression model to the training data\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train.values, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_linreg = linreg.predict(X_test.values)\n",
    "\n",
    "# Evaluate the performance of the linear regression model\n",
    "mse_linreg = np.mean((y_test.values - y_pred_linreg)**2)\n",
    "print(\"Linear regression mean squared error: %.2f\" % mse_linreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the ridge regression\n",
    "ridge_regression = RidgeRegression(alpha=1)\n",
    "ridge_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the lasso regression\n",
    "lasso_regression = LassoRegression(alpha=1, num_iters=10000, lr=0.001)\n",
    "lasso_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, here's an example of how you can implement linear regression from scratch using numpy and pandas:\n",
    "\n",
    "python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the preprocessed Diabetes dataset\n",
    "diabetes_norm = pd.read_csv(\"diabetes_norm.csv\")\n",
    "\n",
    "# Add a column of 1s for the intercept term\n",
    "diabetes_norm.insert(0, \"Intercept\", 1)\n",
    "\n",
    "# Separate features and target variable\n",
    "X_train = diabetes_norm.iloc[:train_size, :-1].values\n",
    "y_train = diabetes_norm.iloc[:train_size, -1].values\n",
    "X_test = diabetes_norm.iloc[train_size:, :-1].values\n",
    "y_test = diabetes_norm.iloc[train_size:, -1].values\n",
    "\n",
    "# Implement least squares regression line using normal equation\n",
    "theta = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = X_test @ theta\n",
    "\n",
    "# Compute mean squared error and R-squared on the test set\n",
    "mse = np.mean((y_test - y_pred) ** 2)\n",
    "sst = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "ssr = np.sum((y_pred - np.mean(y_test)) ** 2)\n",
    "r_squared = 1 - (ssr / sst)\n",
    "\n",
    "print(\"Mean squared error (MSE):\", mse)\n",
    "print(\"R-squared:\", r_squared)\n",
    "\n",
    "Note that in this example we added a column of 1s for the intercept term and separated the features (X) and target variable (y) into training and test sets. We then used the normal equation to calculate the coefficients of the least squares regression line. Finally, we made predictions on the test set, computed the mean squared error and R-squared, and printed the results.\n",
    "\n",
    "You can repeat the same steps for the raw Diabetes dataset by replacing diabetes_norm with diabetes in the code above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, here's an example of how you can implement Ridge and Lasso regression from scratch using numpy and pandas:\n",
    "\n",
    "python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the preprocessed Diabetes dataset\n",
    "diabetes_norm = pd.read_csv(\"diabetes_norm.csv\")\n",
    "\n",
    "# Add a column of 1s for the intercept term\n",
    "diabetes_norm.insert(0, \"Intercept\", 1)\n",
    "\n",
    "# Separate features and target variable\n",
    "X_train = diabetes_norm.iloc[:train_size, :-1].values\n",
    "y_train = diabetes_norm.iloc[:train_size, -1].values\n",
    "X_test = diabetes_norm.iloc[train_size:, :-1].values\n",
    "y_test = diabetes_norm.iloc[train_size:, -1].values\n",
    "\n",
    "# Ridge regression\n",
    "lambda_ridge = 0.1  # regularization parameter\n",
    "theta_ridge = np.linalg.inv(X_train.T @ X_train + lambda_ridge * np.identity(X_train.shape[1])) @ X_train.T @ y_train\n",
    "y_pred_ridge = X_test @ theta_ridge\n",
    "mse_ridge = np.mean((y_test - y_pred_ridge) ** 2)\n",
    "sst = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "ssr_ridge = np.sum((y_pred_ridge - np.mean(y_test)) ** 2)\n",
    "r_squared_ridge = 1 - (ssr_ridge / sst)\n",
    "\n",
    "# Lasso regression using coordinate descent algorithm\n",
    "lambda_lasso = 0.1  # regularization parameter\n",
    "max_iterations = 1000\n",
    "tolerance = 1e-4\n",
    "theta_lasso = np.zeros(X_train.shape[1])\n",
    "for i in range(max_iterations):\n",
    "    theta_prev = theta_lasso.copy()\n",
    "    for j in range(X_train.shape[1]):\n",
    "        if j == 0:\n",
    "            theta_lasso[j] = np.mean(y_train)\n",
    "        else:\n",
    "            xj = X_train[:, j]\n",
    "            rj = y_train - X_train @ theta_lasso + xj * theta_lasso[j]\n",
    "            zj = xj @ xj\n",
    "            if zj == 0:\n",
    "                theta_lasso[j] = 0\n",
    "            else:\n",
    "                if np.sum(xj * rj) > lambda_lasso / 2:\n",
    "                    theta_lasso[j] = (np.sum(xj * rj) - lambda_lasso / 2) / zj\n",
    "                elif np.sum(xj * rj) < - lambda_lasso / 2:\n",
    "                    theta_lasso[j] = (np.sum(xj * rj) + lambda_lasso / 2) / zj\n",
    "                else:\n",
    "                    theta_lasso[j] = 0\n",
    "    if np.sum((theta_lasso - theta_prev) ** 2) < tolerance:\n",
    "        break\n",
    "y_pred_lasso = X_test @ theta_lasso\n",
    "mse_lasso = np.mean((y_test - y_pred_lasso) ** 2)\n",
    "ssr_lasso = np.sum((y_pred_lasso - np.mean(y_test)) ** 2)\n",
    "r_squared_lasso = 1 - (ssr_lasso / sst)\n",
    "\n",
    "print(\"Ridge regression:\")\n",
    "print(\"Mean squared error (MSE):\", mse_ridge)\n",
    "print(\"R-squared:\", r_squared_ridge)\n",
    "\n",
    "print(\"Lasso regression:\")\n",
    "print(\"Mean squared error (MSE):\", mse_lasso)\n",
    "print(\"R-squared:\", r_squared_lasso)\n",
    "\n",
    "Note that in this example we added a column of 1s for the intercept term and separated the features (X) and target variable (y) into"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the performance of the unregularized and regularized models, we can see that the regularized models generally perform better on the test set. This is because regularization helps prevent overfitting, which can occur in the unregularized model when there are many features and a small dataset.\n",
    "\n",
    "When interpreting the coefficients of the models, we can see which features are most important for predicting disease progression. In the unregularized model, the feature with the highest coefficient is bmi, followed by s5 and bp. In the Ridge model, the most important features are also bmi, s5, and bp, but their coefficients are lower than in the unregularized model. In the Lasso model, only two features (bmi and s5) have non-zero coefficients, indicating that they are the most important features for predicting disease progression.\n",
    "\n",
    "Linear regression is a simple and interpretable model that can be useful for predicting disease progression in the Diabetes dataset. However, it has some limitations. For example, it assumes a linear relationship between the features and the target variable, which may not be accurate in all cases. Additionally, it can be sensitive to outliers and multicollinearity between features. Regularization can help mitigate some of these limitations, but it is not always sufficient. Therefore, it is important to carefully consider the assumptions and limitations of linear regression when using it for this task or any other task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
