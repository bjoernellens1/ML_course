{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# Assignment 4 of the course “Introduction to Machine Learning” at the University of Leoben.\n",
    "\n",
    "Author: Fotios Lygerakis\n",
    "\n",
    "Editor: Björn Ellensohn\n",
    "\n",
    "Semester: SS 2022/2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is about solving an assignment for linear regression models. The task has to be solved without using sklearn and the following models where implemented from scratch:\n",
    "- Linear Regression\n",
    "    * Least Squares Regression\n",
    "- Regularized Linear Regression\n",
    "    * Ridge Regression\n",
    "    * Lasso Regression (Bonus Task)\n",
    "\n",
    "A template notebook was given outlining the structure needed to solve the exercise. But for my conveniance, I implemented my own procecudere on solving the assignment. I rather liked the idea of having the classes already doing the data processing on class initialization. That way it was easier for me to understand the different models and compare them.\n",
    "\n",
    "\n",
    "##  Parent Class\n",
    "So first I extended the parent class Predictor with the appropriate methods.\n",
    "- Reading the csv\n",
    "- Loading and splitting the data into training and test sets\n",
    "- Normalizing the data\n",
    "\n",
    "was the same for all the models, so the code for that moved into the parent class. Also I decided to define extra attributes for the pre-processed dataset and the raw dataset.\n",
    "\n",
    "In the preprocess() method, I used the \"1.5 x IQR rule\" to find the outliers in the dataset. This can be nicely displayed by a boxplot. Those values then got handled by replacing them with the median value of the corresponding column. Also, missing values and zeros should be replaced, but in our case, none of them where found, as far as I know. \n",
    "\n",
    "The train_test_split() method, on the other hand, helps me to divide the dataset into the features and the target. Furthermore, the whole dataset gets split into training data and testing data by a factor of 0.8 to 0.2. This is used later for evaluating the models' performance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Start Coding\n",
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create the Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, dataset):\n",
    "        self.coefficients = None\n",
    "        self.df = pd.read_csv(dataset)\n",
    "        self.df_pp = self.preprocess(self.df)\n",
    "        self.X_train_raw, self.X_test_raw, self.y_train_raw, self.y_test_raw = self.train_test_split(self.df)\n",
    "        self.X_train_pp, self.X_test_pp, self.y_train_pp, self.y_test_pp = self.train_test_split(self.df_pp)\n",
    "\n",
    "    def preprocess(self, df):\n",
    "        # It is better to do this after splitting the dataset. --> need to change that\n",
    "        # Handle missing values\n",
    "        df.replace(0, np.nan, inplace=True)\n",
    "\n",
    "        # Remove outliers using iqr rule:\n",
    "        df_cleaned = df.copy()\n",
    "        for column in df:\n",
    "            q1 = df[column].quantile(q=0.25)\n",
    "            q3 = df[column].quantile(q=0.75)\n",
    "            med = df[column].median()\n",
    "\n",
    "            iqr = q3 - q1\n",
    "            upper_bound = q3+(1.5*iqr)\n",
    "            lower_bound = q1-(1.5*iqr)\n",
    "\n",
    "            df_cleaned[column][(df[column] <= lower_bound) | (df[column] >= upper_bound)] = df_cleaned[column].median()\n",
    "\n",
    "        # Normalize data:\n",
    "        df_normalized = df_cleaned.copy()\n",
    "        for column in df_cleaned:\n",
    "\n",
    "            mean = df_cleaned[column].mean()\n",
    "            std = df_cleaned[column].std()\n",
    "\n",
    "            df_normalized[column] = (df_cleaned[column] - mean) / std\n",
    "        df_processed = df_normalized\n",
    "        return df_processed\n",
    "\n",
    "    def train_test_split(self, df, test_size=0.2):\n",
    "        # Shuffle the rows of the dataset randomly\n",
    "        df_randomized = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "        # Extract the features and target variable\n",
    "        X = df_randomized.drop('target', axis=1)\n",
    "        y = df_randomized['target']\n",
    "\n",
    "        # Split the dataset into training and testing sets\n",
    "        split_ratio = 1 - test_size\n",
    "        split_index = int(split_ratio * len(df_randomized))\n",
    "\n",
    "        X_train = X[:split_index]\n",
    "        X_test = X[split_index:]\n",
    "        y_train = y[:split_index]\n",
    "        y_test = y[split_index:]\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Child Classes\n",
    "Code for the corresponding model.\n",
    "\n",
    "### Linear Regression - Least Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(Predictor):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__(dataset)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        y = y.values.reshape(-1, 1)\n",
    "        self.coefficients = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        return X.dot(self.coefficients)\n",
    "    \n",
    "    def regression_raw(self):\n",
    "        # copy the values\n",
    "        X_train = self.X_train_raw\n",
    "        y_train = self.y_train_raw\n",
    "        X_test = self.X_test_raw\n",
    "        y_test = self.y_test_raw\n",
    "\n",
    "        # implement linear regression\n",
    "\n",
    "        # Implement the formula for the least-squares regression line\n",
    "        X_train_T = np.transpose(X_train)\n",
    "        beta = np.linalg.inv(X_train_T.dot(X_train)).dot(X_train_T).dot(y_train) # these are the weights\n",
    "\n",
    "        # Train the model on the training set using the least-squares regression line\n",
    "        y_pred_train = X_train.dot(beta) # the prediction on the train set\n",
    "\n",
    "        # Evaluate the performance of the model on the testing set using metrics such as mean squared error and R-squared\n",
    "        y_pred_test = X_test.dot(beta) # the prediction on the test set\n",
    "\n",
    "        # calculate the mean squared error\n",
    "        mse = np.mean((y_test - y_pred_test)**2)\n",
    "        r_squared = 1 - (np.sum((y_test - y_pred_test)**2) / np.sum((y_test - np.mean(y_test))**2))\n",
    "\n",
    "        print('Mean squared error:', mse)\n",
    "        print('R-squared:', r_squared)\n",
    "\n",
    "        # that should do\n",
    "\n",
    "    def regression_pp(self):\n",
    "        # copy the values\n",
    "        X_train = self.X_train_pp\n",
    "        y_train = self.y_train_pp\n",
    "        X_test = self.X_test_pp\n",
    "        y_test = self.y_test_pp\n",
    "        \n",
    "        # implement linear regression\n",
    "\n",
    "        # Implement the formula for the least-squares regression line\n",
    "        X_train_T = np.transpose(X_train)\n",
    "        beta = np.linalg.inv(X_train_T.dot(X_train)).dot(X_train_T).dot(y_train) # these are the weights\n",
    "\n",
    "        # Train the model on the training set using the least-squares regression line\n",
    "        y_pred_train = X_train.dot(beta) # the prediction on the train set\n",
    "\n",
    "        # Evaluate the performance of the model on the testing set using metrics such as mean squared error and R-squared\n",
    "        y_pred_test = X_test.dot(beta) # the prediction on the test set\n",
    "\n",
    "        # calculate the mean squared error\n",
    "        mse = np.mean((y_test - y_pred_test)**2)\n",
    "        r_squared = 1 - (np.sum((y_test - y_pred_test)**2) / np.sum((y_test - np.mean(y_test))**2))\n",
    "\n",
    "        print('Mean squared error:', mse)\n",
    "        print('R-squared:', r_squared)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear Regression - Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression(Predictor):\n",
    "    def __init__(self, dataset, alpha=1):\n",
    "        super().__init__(dataset) # this command initializes the parent class (Predictor) and passes the dataset.\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def regression_raw(self):\n",
    "        \"\"\"\n",
    "        Fits a ridge regression model on the training data using the specified regularization parameter alpha.\n",
    "        Using raw dataset\n",
    "        \"\"\"\n",
    "        X_train = self.X_train_raw\n",
    "        X_test = self.X_test_raw\n",
    "        y_train = self.y_train_raw\n",
    "        y_test = self.y_test_raw\n",
    "        alpha = self.alpha\n",
    "        \n",
    "        # Add a cloumn of 1s to the training data to have the correct dimension.\n",
    "        X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "\n",
    "        n_features = X_train.shape[1]\n",
    "        I = np.eye(n_features)\n",
    "        w = np.linalg.inv(X_train.T.dot(X_train) + alpha * I).dot(X_train.T).dot(y_train)\n",
    "\n",
    "        #X_test = (X_test - X.mean()) / X.std() #this should not be needed, since the normalization is happening in the constructor\n",
    "        X_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
    "        y_pred = X_test.dot(w)\n",
    "\n",
    "        # calculate the mean squared error\n",
    "        mse = np.mean((y_test - y_pred)**2)\n",
    "        r_squared = 1 - (np.sum((y_test - y_pred)**2) / np.sum((y_test - np.mean(y_test))**2))\n",
    "\n",
    "        # calculate root mean squared error (RMSE)\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        print(\"Predicted target value:\", y_pred[0])\n",
    "        print(\"Mean squared error (MSE):\", mse)\n",
    "        print(\"R-squared:\", r_squared)\n",
    "        print(\"Root mean squared error (RMSE):\", rmse)\n",
    "\n",
    "    def regression_pp(self):\n",
    "        \"\"\"\n",
    "        Fits a ridge regression model on the training data using the specified regularization parameter alpha.\n",
    "        Using processed dataset.\n",
    "        \"\"\"\n",
    "        X_train = self.X_train_pp\n",
    "        X_test = self.X_test_pp\n",
    "        y_train = self.y_train_pp\n",
    "        y_test = self.y_test_pp\n",
    "        alpha = self.alpha\n",
    "        \n",
    "        # Add a cloumn of 1s to the training data to have the correct dimension.\n",
    "        X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "\n",
    "        n_features = X_train.shape[1]\n",
    "        I = np.eye(n_features)\n",
    "        w = np.linalg.inv(X_train.T.dot(X_train) + alpha * I).dot(X_train.T).dot(y_train)\n",
    "\n",
    "        X_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
    "        y_pred = X_test.dot(w)\n",
    "\n",
    "        # calculate the mean squared error\n",
    "        mse = np.mean((y_test - y_pred)**2)\n",
    "        r_squared = 1 - (np.sum((y_test - y_pred)**2) / np.sum((y_test - np.mean(y_test))**2))\n",
    "\n",
    "        # calculate root mean squared error (RMSE)\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        print(\"Predicted target value:\", y_pred[0])\n",
    "        print(\"Mean squared error (MSE):\", mse)\n",
    "        print(\"R-squared:\", r_squared)\n",
    "        print(\"Root mean squared error (RMSE):\", rmse)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear Regression - Lasso Regression (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoRegression(Predictor):\n",
    "    def __init__(self, dataset, alpha=1, max_iter=1000, tol=0.0001):\n",
    "        super().__init__(dataset)\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def regression_raw(self):\n",
    "        \"\"\"\n",
    "        Fits a lasso regression model on the training data using the specified regularization parameter alpha, iterations, and tolerance.\n",
    "        Using raw dataset.\n",
    "        \"\"\"\n",
    "        # Define hyperparameters\n",
    "        alpha = self.alpha  # regularization strength\n",
    "        max_iterations = self.max_iter  # number of gradient descent iterations\n",
    "        tolerance = self.tol\n",
    "\n",
    "        # Load the data\n",
    "        diabetes = pd.read_csv(\"diabetes.csv\")\n",
    "\n",
    "        diabetes.insert(0, \"Intercept\", 1)\n",
    "\n",
    "        train_size = int(0.8 * len(diabetes))\n",
    "        \n",
    "        X_train = diabetes.iloc[:train_size, :-1].values\n",
    "        y_train = diabetes.iloc[:train_size, -1].values\n",
    "        X_test = diabetes.iloc[train_size:, :-1].values\n",
    "        y_test = diabetes.iloc[train_size:, -1].values\n",
    "\n",
    "        theta_lasso = np.zeros(X_train.shape[1])\n",
    "        for i in range(max_iterations):\n",
    "            theta_prev = theta_lasso.copy()\n",
    "            for j in range(X_train.shape[1]):\n",
    "                if j == 0:\n",
    "                    theta_lasso[j] = np.mean(y_train)\n",
    "                else:\n",
    "                    xj = X_train[:, j]\n",
    "                    rj = y_train - X_train @ theta_lasso + xj * theta_lasso[j]\n",
    "                    zj = xj @ xj\n",
    "                    if zj == 0:\n",
    "                        theta_lasso[j] = 0\n",
    "                    else:\n",
    "                        if np.sum(xj * rj) > alpha / 2:\n",
    "                            theta_lasso[j] = (np.sum(xj * rj) - alpha / 2) / zj\n",
    "                        elif np.sum(xj * rj) < - alpha / 2:\n",
    "                            theta_lasso[j] = (np.sum(xj * rj) + alpha / 2) / zj\n",
    "                        else:\n",
    "                            theta_lasso[j] = 0\n",
    "            if np.sum((theta_lasso - theta_prev) ** 2) < tolerance:\n",
    "                break\n",
    "        \n",
    "        sst = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "\n",
    "        y_pred_lasso = X_test @ theta_lasso\n",
    "        mse_lasso = np.mean((y_test - y_pred_lasso) ** 2)\n",
    "        ssr_lasso = np.sum((y_pred_lasso - np.mean(y_test)) ** 2)\n",
    "        r_squared_lasso = 1 - (ssr_lasso / sst)\n",
    "\n",
    "        rmse_lasso = np.sqrt(mse_lasso)\n",
    "\n",
    "        print(\"Lasso regression:\")\n",
    "        print(\"Mean squared error (MSE):\", mse_lasso)\n",
    "        print(\"R-squared:\", r_squared_lasso)\n",
    "        print(\"Root mean sqaured error (RMSE):\", rmse_lasso)\n",
    "\n",
    "    def regression_pp(self):\n",
    "        \"\"\"\n",
    "        Fits a lasso regression model on the training data using the specified regularization parameter alpha, iterations, and tolerance.\n",
    "        Using processed dataset.\n",
    "        \"\"\"\n",
    "        # Define hyperparameters\n",
    "        alpha = self.alpha  # regularization strength\n",
    "        max_iterations = self.max_iter  # number of gradient descent iterations\n",
    "        tolerance = self.tol\n",
    "\n",
    "        # Load the data\n",
    "        diabetes_norm = pd.read_csv(\"diabetes_norm.csv\")\n",
    "\n",
    "        diabetes_norm.insert(0, \"Intercept\", 1)\n",
    "\n",
    "        train_size = int(0.8 * len(diabetes_norm))\n",
    "\n",
    "        X_train = diabetes_norm.iloc[:train_size, :-1].values\n",
    "        y_train = diabetes_norm.iloc[:train_size, -1].values\n",
    "        X_test = diabetes_norm.iloc[train_size:, :-1].values\n",
    "        y_test = diabetes_norm.iloc[train_size:, -1].values\n",
    "\n",
    "        theta_lasso = np.zeros(X_train.shape[1])\n",
    "        for i in range(max_iterations):\n",
    "            theta_prev = theta_lasso.copy()\n",
    "            for j in range(X_train.shape[1]):\n",
    "                if j == 0:\n",
    "                    theta_lasso[j] = np.mean(y_train)\n",
    "                else:\n",
    "                    xj = X_train[:, j]\n",
    "                    rj = y_train - X_train @ theta_lasso + xj * theta_lasso[j]\n",
    "                    zj = xj @ xj\n",
    "                    if zj == 0:\n",
    "                        theta_lasso[j] = 0\n",
    "                    else:\n",
    "                        if np.sum(xj * rj) > alpha / 2:\n",
    "                            theta_lasso[j] = (np.sum(xj * rj) - alpha / 2) / zj\n",
    "                        elif np.sum(xj * rj) < - alpha / 2:\n",
    "                            theta_lasso[j] = (np.sum(xj * rj) + alpha / 2) / zj\n",
    "                        else:\n",
    "                            theta_lasso[j] = 0\n",
    "            if np.sum((theta_lasso - theta_prev) ** 2) < tolerance:\n",
    "                break\n",
    "        \n",
    "        sst = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "\n",
    "        y_pred_lasso = X_test @ theta_lasso\n",
    "        mse_lasso = np.mean((y_test - y_pred_lasso) ** 2)\n",
    "        ssr_lasso = np.sum((y_pred_lasso - np.mean(y_test)) ** 2)\n",
    "        r_squared_lasso = 1 - (ssr_lasso / sst)\n",
    "\n",
    "        rmse_lasso = np.sqrt(mse_lasso)\n",
    "\n",
    "        print(\"Lasso regression:\")\n",
    "        print(\"Mean squared error (MSE):\", mse_lasso)\n",
    "        print(\"R-squared:\", r_squared_lasso)\n",
    "        print(\"Root mean sqaured error (RMSE):\", rmse_lasso)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 3460.083533525722\n",
      "R-squared: 0.37854964415469516\n"
     ]
    }
   ],
   "source": [
    "rg = LinearRegression('diabetes.csv')\n",
    "# Raw Dataset\n",
    "rg.regression_raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.5359087499312691\n",
      "R-squared: 0.42794122908248045\n"
     ]
    }
   ],
   "source": [
    "# Pre-processed Dataset\n",
    "rg.regression_pp()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted target value: 208.82139795491315\n",
      "Mean squared error (MSE): 3369.602933909638\n",
      "R-squared: 0.39480046592928286\n",
      "Root mean squared error (RMSE): 58.04828105904289\n"
     ]
    }
   ],
   "source": [
    "rr = RidgeRegression('diabetes.csv')\n",
    "# Raw Dataset\n",
    "rr.regression_raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted target value: 0.7839100243805647\n",
      "Mean squared error (MSE): 0.5407373522105846\n",
      "R-squared: 0.42278691076707886\n",
      "Root mean squared error (RMSE): 0.7353484563188969\n"
     ]
    }
   ],
   "source": [
    "# Pre-processed Dataset\n",
    "rr.regression_pp()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso regression:\n",
      "Mean squared error (MSE): 3574.2278339827644\n",
      "R-squared: 0.5172053348571604\n",
      "Root mean sqaured error (RMSE): 59.784846190174015\n"
     ]
    }
   ],
   "source": [
    "lr = LassoRegression('diabetes.csv')\n",
    "# Raw Dataset\n",
    "lr.regression_raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso regression:\n",
      "Mean squared error (MSE): 0.5080018069655786\n",
      "R-squared: 0.5404721520708679\n",
      "Root mean sqaured error (RMSE): 0.7127424548640123\n"
     ]
    }
   ],
   "source": [
    "# Pre-processed Dataset\n",
    "lr.regression_pp()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
