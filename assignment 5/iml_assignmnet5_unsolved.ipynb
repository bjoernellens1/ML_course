{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Perceptron Algorithm for Classification of Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in /usr/local/python/3.10.4/lib/python3.10/site-packages (2.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ToDo: split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=None, shuffle=True, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'iris.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define the perceptron algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, the forward() method calculates the weighted sum of inputs and biases, applies the softmax function to obtain class probabilities, and returns the probabilities. The backward() method calculates the gradients of the weights and biases using the predicted probabilities and the true labels, and then updates the parameters using gradient descent. The predict() method uses the forward() method to get the class probabilities and selects the class with the highest probability as the predicted class for each example.\n",
    "\n",
    "Note that this implementation assumes that the input X is a 2-dimensional numpy array of shape (num_examples, input_dim) and the target y is a one-hot encoded array of shape (num_examples, output_dim). Adjust the code accordingly if your data has a different format.\n",
    "\n",
    "self.lr is the learning rate. Meaning the step size."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt to shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the perceptron algorithm\n",
    "class MultiClassPerceptron:\n",
    "    def __init__(self, input_dim, output_dim, lr=0.01, epochs=1000):\n",
    "        self.W = np.random.randn(input_dim, output_dim)\n",
    "        self.b = np.zeros((1, output_dim))\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def forward(self, X):\n",
    "        weighted_sum = np.dot(X, self.W) + self.b\n",
    "        probabilities = np.exp(weighted_sum) / np.sum(np.exp(weighted_sum), axis=1, keepdims=True)\n",
    "        return probabilities\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        probabilities = self.forward(X)\n",
    "        # Convert y to one-hot encoded form\n",
    "        y_one_hot = np.eye(self.W.shape[1])[y]\n",
    "\n",
    "        dW = (1 / m) * np.dot(X.T, (probabilities - y_one_hot))\n",
    "        db = (1 / m) * np.sum(probabilities - y_one_hot, axis=0)\n",
    "\n",
    "        self.W -= self.lr * dW\n",
    "        self.b -= self.lr * db\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.forward(X)\n",
    "            self.backward(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probabilities = self.forward(X)\n",
    "        predictions = np.argmax(probabilities, axis=1)\n",
    "        return predictions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "p = MultiClassPerceptron(input_dim=X_train.shape[1], output_dim=3, lr=0.0005, epochs=1000)\n",
    "p.fit(X_train, y_train)\n",
    "predictions_train = p.predict(X_train)\n",
    "predictions = p.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron classification train accuracy 0.4\n",
      "Perceptron classification accuracy 0.26666666666666666\n"
     ]
    }
   ],
   "source": [
    "# evaluate train accuracy\n",
    "print(\"Perceptron classification train accuracy\", accuracy_score(y_train, predictions_train))\n",
    "print(\"Perceptron classification accuracy\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Non-linear feature transformation on the concrete compressive strength dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "# ToDo: implement the polynomial_features() function\n",
    "def polynomial_features(X, degree):\n",
    "    n_samples, n_features = X.shape\n",
    "    polynomial_X = []\n",
    "    \n",
    "    for comb in combinations_with_replacement(range(n_features), degree):\n",
    "        poly_features = X[:, comb[0]]\n",
    "        for feature_idx in comb[1:]:\n",
    "            poly_features *= X[:, feature_idx]\n",
    "        polynomial_X.append(poly_features)\n",
    "    \n",
    "    polynomial_X = np.column_stack(polynomial_X)\n",
    "    return polynomial_X\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, the polynomial_features() function takes two arguments: X, which represents the input features as a 2-dimensional numpy array of shape (n_samples, n_features), and degree, which specifies the degree of the polynomial features to generate.\n",
    "\n",
    "The function iterates over the combinations with replacement of feature indices up to the specified degree using combinations_with_replacement(). For each combination, it creates a new array of polynomial features by multiplying the corresponding columns of X. The resulting polynomial features are appended to the polynomial_X list.\n",
    "\n",
    "Finally, the list of polynomial features is stacked horizontally using np.column_stack() to form the final polynomial_X array, which is then returned.\n",
    "\n",
    "You can use this polynomial_features() function to generate polynomial features before applying the linear regression model or any other machine learning algorithm that requires non-linear feature transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "# Implement the polynomial_features() function\n",
    "def polynomial_features(X, degree):\n",
    "    n_samples, n_features = X.shape\n",
    "    polynomial_X = np.ones((n_samples, 1))\n",
    "\n",
    "    for d in range(1, degree + 1):\n",
    "        combinations = combinations_with_replacement(range(n_features), d)\n",
    "        for comb in combinations:\n",
    "            poly_features = np.prod(X[:, comb], axis=1, keepdims=True)\n",
    "            polynomial_X = np.hstack((polynomial_X, poly_features))\n",
    "\n",
    "    return polynomial_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Non-linear feature transformation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# load the concrete compressive strength dataset\n",
    "df = pd.read_excel('Concrete_Data.xls')\n",
    "\n",
    "# Selecting the Concrete compressive strength as targets, rest is features\n",
    "X = df.drop('Concrete compressive strength(MPa, megapascals) ', axis=1)\n",
    "y = df['Concrete compressive strength(MPa, megapascals) ']\n",
    "\n",
    "\n",
    "# ToDo: split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=None, shuffle=True, stratify=None)\n",
    "\n",
    "# transform the features into second degree polynomial features\n",
    "X_train_poly_custom = polynomial_features(X_train.values, degree=2)\n",
    "X_test_poly_custom = polynomial_features(X_test.values, degree=2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Train the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (train poly custom): 53.18\n",
      "Mean squared error (test poly custom): 54.87\n",
      "Mean squared error (train): 109.21\n",
      "Mean squared error (test): 100.21\n",
      "R^2 (train poly custom): 0.81\n",
      "R^2 (test poly custom): 0.80\n",
      "R^2 (train): 0.61\n",
      "R^2 (test): 0.64\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "lr_poly_custom = LinearRegression()\n",
    "lr = LinearRegression()\n",
    "# fit the model\n",
    "lr_poly_custom.fit(X_train_poly_custom, y_train)\n",
    "lr.fit(X_train, y_train)\n",
    "# predict values from the polynomial transformed features\n",
    "predictions_poly_custom_train = lr_poly_custom.predict(X_train_poly_custom)\n",
    "predictions_poly_custom = lr_poly_custom.predict(X_test_poly_custom)\n",
    "# predict values from the original features\n",
    "predictions_train = lr.predict(X_train)\n",
    "predictions = lr.predict(X_test)\n",
    "\n",
    "# mean squared error\n",
    "print(\"Mean squared error (train poly custom): {:.2f}\".format(mean_squared_error(y_train, predictions_poly_custom_train)))\n",
    "print(\"Mean squared error (test poly custom): {:.2f}\".format(mean_squared_error(y_test, predictions_poly_custom)))\n",
    "print(\"Mean squared error (train): {:.2f}\".format(mean_squared_error(y_train, predictions_train)))\n",
    "print(\"Mean squared error (test): {:.2f}\".format(mean_squared_error(y_test, predictions)))\n",
    "\n",
    "# coefficient of determination (R^2)\n",
    "print(\"R^2 (train poly custom): {:.2f}\".format(r2_score(y_train, predictions_poly_custom_train)))\n",
    "print(\"R^2 (test poly custom): {:.2f}\".format(r2_score(y_test, predictions_poly_custom)))\n",
    "print(\"R^2 (train): {:.2f}\".format(r2_score(y_train, predictions_train)))\n",
    "print(\"R^2 (test): {:.2f}\".format(r2_score(y_test, predictions)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "RBFs on the California Housing Prices dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ToDo: implement the rbf_kernel() function\n",
    "def rbf_kernel(X, centers, gamma):\n",
    "    n_samples = X.shape[0]\n",
    "    n_centers = centers.shape[0]\n",
    "    K = np.zeros((n_samples, n_centers))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_centers):\n",
    "            diff = X[i] - centers[j]\n",
    "            K[i, j] = np.exp(-gamma * np.dot(diff, diff))\n",
    "\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression on original data:\n",
      "MSE: 0.5558915986952443\n",
      "R^2: 0.5757877060324508\n",
      "\n",
      "Linear regression on RBF-transformed data:\n",
      "MSE: 0.37106446913117447\n",
      "R^2: 0.7168330839511696\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the California Housing Prices dataset\n",
    "data = fetch_california_housing()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# ToDo: split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=None, shuffle=True, stratify=None)\n",
    "\n",
    "# ToDo: standardize the data\n",
    "X_train_std = StandardScaler(X_train)\n",
    "X_test_std = StandardScaler(X_test)\n",
    "\n",
    "# Choose the number of centroids and the RBF kernel width\n",
    "num_centroids = 100\n",
    "gamma = 0.1\n",
    "\n",
    "# Randomly select the centroids from the training set\n",
    "np.random.seed(42)\n",
    "idx = np.random.choice(X_train_std.shape[0], num_centroids, replace=False)\n",
    "centroids = X_train_std[idx]\n",
    "\n",
    "# Compute the RBF features for the training and testing sets\n",
    "rbf_train = rbf_kernel(X_train_std, centroids, gamma)\n",
    "rbf_test = rbf_kernel(X_test_std, centroids, gamma)\n",
    "\n",
    "# Fit a linear regression model on the original and RBF-transformed data\n",
    "linreg_orig = LinearRegression().fit(X_train_std, y_train)\n",
    "linreg_rbf = LinearRegression().fit(rbf_train, y_train)\n",
    "\n",
    "# Evaluate the models on the testing set\n",
    "y_pred_orig = linreg_orig.predict(X_test_std)\n",
    "mse_orig = mean_squared_error(y_test, y_pred_orig)\n",
    "r2_orig = r2_score(y_test, y_pred_orig)\n",
    "\n",
    "y_pred_rbf = linreg_rbf.predict(rbf_test)\n",
    "mse_rbf = mean_squared_error(y_test, y_pred_rbf)\n",
    "r2_rbf = r2_score(y_test, y_pred_rbf)\n",
    "\n",
    "# Print the results\n",
    "print(\"Linear regression on original data:\")\n",
    "print(\"MSE:\", mse_orig)\n",
    "print(\"R^2:\", r2_orig)\n",
    "\n",
    "print(\"\\nLinear regression on RBF-transformed data:\")\n",
    "print(\"MSE:\", mse_rbf)\n",
    "print(\"R^2:\", r2_rbf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression on original data:\n",
      "MSE: 0.5475175660043198\n",
      "R^2: 0.5986485521339906\n",
      "\n",
      "Linear regression on RBF-transformed data:\n",
      "MSE: 0.37921747143043055\n",
      "R^2: 0.7220189987228838\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the California Housing Prices dataset\n",
    "data = fetch_california_housing()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# ToDo: split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=None, shuffle=True, stratify=None)\n",
    "\n",
    "# ToDo: standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.fit_transform(X_test)\n",
    "\n",
    "# Choose the number of centroids and the RBF kernel width\n",
    "num_centroids = 100\n",
    "gamma = 0.1\n",
    "\n",
    "# Randomly select the centroids from the training set\n",
    "np.random.seed(42)\n",
    "idx = np.random.choice(X_train_std.shape[0], num_centroids, replace=False)\n",
    "centroids = X_train_std[idx]\n",
    "\n",
    "# Compute the RBF features for the training and testing sets\n",
    "rbf_train = rbf_kernel(X_train_std, centroids, gamma)\n",
    "rbf_test = rbf_kernel(X_test_std, centroids, gamma)\n",
    "\n",
    "# Fit a linear regression model on the original and RBF-transformed data\n",
    "linreg_orig = LinearRegression().fit(X_train_std, y_train)\n",
    "linreg_rbf = LinearRegression().fit(rbf_train, y_train)\n",
    "\n",
    "# Evaluate the models on the testing set\n",
    "y_pred_orig = linreg_orig.predict(X_test_std)\n",
    "mse_orig = mean_squared_error(y_test, y_pred_orig)\n",
    "r2_orig = r2_score(y_test, y_pred_orig)\n",
    "\n",
    "y_pred_rbf = linreg_rbf.predict(rbf_test)\n",
    "mse_rbf = mean_squared_error(y_test, y_pred_rbf)\n",
    "r2_rbf = r2_score(y_test, y_pred_rbf)\n",
    "\n",
    "# Print the results\n",
    "print(\"Linear regression on original data:\")\n",
    "print(\"MSE:\", mse_orig)\n",
    "print(\"R^2:\", r2_orig)\n",
    "\n",
    "print(\"\\nLinear regression on RBF-transformed data:\")\n",
    "print(\"MSE:\", mse_rbf)\n",
    "print(\"R^2:\", r2_rbf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **(Bonus)** Multilayer Perceptron Algorithm for Regression of Concrete Compressive Strength Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Download the Concrete Compressive Strength Dataset from the UCI Machine Learning Repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download the Concrete Compressive Strength Dataset from the UCI Machine Learning Repository.\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "concrete = pd.read_excel('Concrete_Data.xls')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Selecting the Concrete compressive strength as targets, rest is features\n",
    "X = concrete.drop('Concrete compressive strength(MPa, megapascals) ', axis=1)\n",
    "y = concrete['Concrete compressive strength(MPa, megapascals) ']\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "# --> maybe remove zeros and outliers\n",
    "\n",
    "\n",
    "# ToDo: normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "#X_test_std = scaler.fit_transform(X_test)\n",
    "\n",
    "#ToDo: split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.2, train_size=0.8, random_state=None, shuffle=True, stratify=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define the multilayer perceptron algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ToDo: Implement the functions in the MLP class\n",
    "class MLP:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.01, epochs=1000):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.01, epochs=1000):\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim)\n",
    "        self.b1 = np.zeros((1, hidden_dim))\n",
    "        self.W2 = np.random.randn(hidden_dim, output_dim)\n",
    "        self.b2 = np.zeros((1, output_dim))\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def forward(self, X):\n",
    "        # ToDo: Implement the forward pass\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = np.tanh(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.y_hat = self.z2\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        # ToDo: Implement the backward pass\n",
    "        m = X.shape[0]\n",
    "        delta2 = (1 / m) * (self.y_hat - y)\n",
    "        dW2 = np.dot(self.a1.T, delta2)\n",
    "        db2 = np.sum(delta2, axis=0, keepdims=True)\n",
    "        delta1 = np.dot(delta2, self.W2.T) * (1 - np.power(self.a1, 2))\n",
    "        dW1 = np.dot(X.T, delta1)\n",
    "        db1 = np.sum(delta1, axis=0)\n",
    "\n",
    "        # Update parameters\n",
    "        self.W2 -= self.lr * dW2\n",
    "        self.b2 -= self.lr * db2\n",
    "        self.W1 -= self.lr * dW1\n",
    "        self.b1 -= self.lr * db1\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.forward(X)\n",
    "            self.backward(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.forward(X)\n",
    "        return self.y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.01, epochs=1000):\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim)\n",
    "        self.b1 = np.zeros((1, hidden_dim))\n",
    "        self.W2 = np.random.randn(hidden_dim, output_dim)\n",
    "        self.b2 = np.zeros((1, output_dim))\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def forward(self, X):\n",
    "        # ToDo: Implement the forward pass\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = np.tanh(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.y_hat = self.z2\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        # ToDo: Implement the backward pass\n",
    "        m = X.shape[0]\n",
    "        delta2 = (1 / m) * (self.y_hat - y)\n",
    "        dW2 = np.dot(self.a1.T, delta2)\n",
    "        db2 = np.sum(delta2, axis=0, keepdims=True)\n",
    "        delta1 = np.dot(delta2, self.W2.T) * (1 - np.power(self.a1, 2))\n",
    "        dW1 = np.dot(X.T, delta1)\n",
    "        db1 = np.sum(delta1, axis=0)\n",
    "\n",
    "        # Update parameters\n",
    "        self.W2 -= self.lr * dW2\n",
    "        self.b2 -= self.lr * db2\n",
    "        self.W1 -= self.lr * dW1\n",
    "        self.b1 -= self.lr * db1\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.forward(X)\n",
    "            self.backward(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.forward(X)\n",
    "        return self.y_hat.flatten()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional, got ndarray of shape (824, 824) instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m mlp \u001b[39m=\u001b[39m MLP(input_dim\u001b[39m=\u001b[39mX_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], hidden_dim\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, output_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, epochs\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m mlp\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "Cell \u001b[0;32mIn[9], line 38\u001b[0m, in \u001b[0;36mMLP.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs):\n\u001b[1;32m     37\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(X)\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackward(X, y)\n",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m, in \u001b[0;36mMLP.backward\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, X, y):\n\u001b[1;32m     20\u001b[0m     \u001b[39m# ToDo: Implement the backward pass\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     m \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 22\u001b[0m     delta2 \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m m) \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my_hat \u001b[39m-\u001b[39;49m y)\n\u001b[1;32m     23\u001b[0m     dW2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma1\u001b[39m.\u001b[39mT, delta2)\n\u001b[1;32m     24\u001b[0m     db2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(delta2, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py:2016\u001b[0m, in \u001b[0;36mNDFrame.__array_ufunc__\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[1;32m   2013\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array_ufunc__\u001b[39m(\n\u001b[1;32m   2014\u001b[0m     \u001b[39mself\u001b[39m, ufunc: np\u001b[39m.\u001b[39mufunc, method: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39minputs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[1;32m   2015\u001b[0m ):\n\u001b[0;32m-> 2016\u001b[0m     \u001b[39mreturn\u001b[39;00m arraylike\u001b[39m.\u001b[39;49marray_ufunc(\u001b[39mself\u001b[39;49m, ufunc, method, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/arraylike.py:273\u001b[0m, in \u001b[0;36marray_ufunc\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m kwargs \u001b[39m=\u001b[39m _standardize_out_kwarg(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    272\u001b[0m \u001b[39m# for binary ops, use our custom dunder methods\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m result \u001b[39m=\u001b[39m maybe_dispatch_ufunc_to_dunder_op(\u001b[39mself\u001b[39;49m, ufunc, method, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    274\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m    275\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/ops_dispatch.pyx:113\u001b[0m, in \u001b[0;36mpandas._libs.ops_dispatch.maybe_dispatch_ufunc_to_dunder_op\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/ops/common.py:81\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m     79\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 81\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/arraylike.py:198\u001b[0m, in \u001b[0;36mOpsMixin.__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__rsub__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    197\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__rsub__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m--> 198\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_arith_method(other, roperator\u001b[39m.\u001b[39;49mrsub)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:6113\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_arith_method\u001b[39m(\u001b[39mself\u001b[39m, other, op):\n\u001b[1;32m   6112\u001b[0m     \u001b[39mself\u001b[39m, other \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39malign_method_SERIES(\u001b[39mself\u001b[39m, other)\n\u001b[0;32m-> 6113\u001b[0m     \u001b[39mreturn\u001b[39;00m base\u001b[39m.\u001b[39;49mIndexOpsMixin\u001b[39m.\u001b[39;49m_arith_method(\u001b[39mself\u001b[39;49m, other, op)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/base.py:1350\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1348\u001b[0m     result \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39marithmetic_op(lvalues, rvalues, op)\n\u001b[0;32m-> 1350\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_construct_result(result, name\u001b[39m=\u001b[39;49mres_name)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:3106\u001b[0m, in \u001b[0;36mSeries._construct_result\u001b[0;34m(self, result, name)\u001b[0m\n\u001b[1;32m   3103\u001b[0m \u001b[39m# TODO: result should always be ArrayLike, but this fails for some\u001b[39;00m\n\u001b[1;32m   3104\u001b[0m \u001b[39m#  JSONArray tests\u001b[39;00m\n\u001b[1;32m   3105\u001b[0m dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(result, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 3106\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_constructor(result, index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m   3107\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m)\n\u001b[1;32m   3109\u001b[0m \u001b[39m# Set the result's name after __finalize__ is called because __finalize__\u001b[39;00m\n\u001b[1;32m   3110\u001b[0m \u001b[39m#  would set it back to self.name\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:509\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    507\u001b[0m         data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    508\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 509\u001b[0m     data \u001b[39m=\u001b[39m sanitize_array(data, index, dtype, copy)\n\u001b[1;32m    511\u001b[0m     manager \u001b[39m=\u001b[39m get_option(\u001b[39m\"\u001b[39m\u001b[39mmode.data_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    512\u001b[0m     \u001b[39mif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mblock\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/construction.py:607\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    604\u001b[0m             subarr \u001b[39m=\u001b[39m cast(np\u001b[39m.\u001b[39mndarray, subarr)\n\u001b[1;32m    605\u001b[0m             subarr \u001b[39m=\u001b[39m maybe_infer_to_datetimelike(subarr)\n\u001b[0;32m--> 607\u001b[0m subarr \u001b[39m=\u001b[39m _sanitize_ndim(subarr, data, dtype, index, allow_2d\u001b[39m=\u001b[39;49mallow_2d)\n\u001b[1;32m    609\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(subarr, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m    610\u001b[0m     \u001b[39m# at this point we should have dtype be None or subarr.dtype == dtype\u001b[39;00m\n\u001b[1;32m    611\u001b[0m     dtype \u001b[39m=\u001b[39m cast(np\u001b[39m.\u001b[39mdtype, dtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/construction.py:666\u001b[0m, in \u001b[0;36m_sanitize_ndim\u001b[0;34m(result, data, dtype, index, allow_2d)\u001b[0m\n\u001b[1;32m    664\u001b[0m     \u001b[39mif\u001b[39;00m allow_2d:\n\u001b[1;32m    665\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m--> 666\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    667\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mData must be 1-dimensional, got ndarray of shape \u001b[39m\u001b[39m{\u001b[39;00mdata\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m instead\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    668\u001b[0m     )\n\u001b[1;32m    669\u001b[0m \u001b[39mif\u001b[39;00m is_object_dtype(dtype) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[1;32m    670\u001b[0m     \u001b[39m# i.e. PandasDtype(\"O\")\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     result \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39masarray_tuplesafe(data, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: Data must be 1-dimensional, got ndarray of shape (824, 824) instead"
     ]
    }
   ],
   "source": [
    "# Create an instance of the MLP class\n",
    "mlp = MLP(input_dim=X_train.shape[1], hidden_dim=10, output_dim=1, lr=0.01, epochs=1000)\n",
    "# Train the model\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 36.8911071801165\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Compare the results with the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 95.97548435337708\n"
     ]
    }
   ],
   "source": [
    "# ToDo: fit a linear regression model on the training data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
